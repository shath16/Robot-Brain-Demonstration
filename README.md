# ğŸ¤– AI Robot Brain Simulator

### Simulated Perception Pipeline with YOLO and Sensor Fusion

---

## ğŸ“– Overview

This project implements a simulated autonomous robot decision system that demonstrates how AI perception and sensor data can be combined to guide navigation behaviour.

The system models a **simulated perception pipeline**, inspired by real robotics architectures, where multiple sensor inputs are processed and prioritised to produce safe movement decisions.

Rather than using real hardware or live camera feeds, predefined scenarios simulate environmental conditions, allowing controlled testing and development of robot decision logic.

---

## ğŸ§  Key Concepts

* Simulated perception pipeline
* AI vision framework integration (Ultralytics YOLO)
* Sensor fusion principles
* Rule-based autonomous decision making
* Robotics navigation logic
* Safety-priority control systems

---

## âš™ï¸ Architecture

The system follows a simplified robotics pipeline:

```
Simulated Sensors â†’ Perception Layer â†’ Decision Logic â†’ Robot Action
```

### 1ï¸âƒ£ Perception Layer (Simulated)

* Simulates YOLO-based object detection results.
* Objects include position and distance information.
* Models how a robot would interpret camera-based AI detections.

### 2ï¸âƒ£ Ultrasonic Sensor Simulation

* Provides distance measurements.
* Used as a safety override mechanism.

### 3ï¸âƒ£ Decision Logic

Priority-based behaviour:

* ğŸš¨ Emergency stop when ultrasonic distance is below safety threshold.
* ğŸ›‘ Stop and turn when obstacle detected in front path.
* âœ… Move forward when path is clear.

---

## ğŸ’» Example Output

```
SCENARIO: Chair blocking path
ROBOT'S VISION:
â€¢ chair detected at center (15cm away)

ULTRASONIC SENSOR: 18cm

ROBOT ACTION: EMERGENCY STOP and TURN LEFT
```

---

## ğŸ“ Learning Objectives

* Understanding perception pipelines in robotics
* Applying sensor fusion concepts
* Designing decision-making systems
* Structuring modular Python robotics code
* Preparing for real-world autonomous navigation systems

---

## ğŸš€ Future Improvements

* Real-time camera integration with YOLO detection
* Hardware implementation (Raspberry Pi / Arduino)
* Motor control and physical robot navigation
* ROS-style architecture
* Machine learning-based path planning

---

## ğŸ› ï¸ Installation

```bash
pip install ultralytics
```

---

## â–¶ï¸ Run

```bash
python robot.py
```

---

